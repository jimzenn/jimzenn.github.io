---
title: "The Bernoulli Process"
date: 2019-03-10T14:09:00+08:00
volumes: ["6"]
layout: "note"
issue: 1
weight: 61

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$
    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\dist}[1]{\text{dist}(#1)}$
    $\newcommand{\max}[1]{\text{max}(#1)}$
    $\newcommand{\min}[1]{\text{min}(#1)}$
    $\newcommand{\supr}{\text{sup}}$
    $\newcommand{\infi}{\text{inf}}$
    $\newcommand{\ite}[1]{\text{int}(#1)}$
    $\newcommand{\ext}[1]{\text{ext}(#1)}$
    $\newcommand{\bdry}[1]{\partial #1}$
    $\newcommand{\argmax}[1]{\underset{#1}{\text{argmax }}}$
    $\newcommand{\argmin}[1]{\underset{#1}{\text{argmin }}}$
    $\newcommand{\set}[1]{\left\{#1\right\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\tilde}{\text{~}}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\proj}{\text{proj}}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{\left| #1 \right|}$
    $\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$
    $\newcommand{\pare}[1]{\left(#1\right)}$
    $\newcommand{\brac}[1]{\left[#1\right]}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\limu}[2]{\underset{#1 \to #2}\lim}$
    $\newcommand{\limsupu}[2]{\underset{#1 \to #2}{\lim\supr}}$
    $\newcommand{\liminfu}[2]{\underset{#1 \to #2}{\lim\infi}}$
    $\newcommand{\limd}[3]{\underset{#1 \to #2; #3}\lim}$
    $\newcommand{\der}[2]{\frac{\d #1}{\d #2}}$
    $\newcommand{\derw}[2]{\frac{\d #1^2}{\d^2 #2}}$
    $\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}$
    $\newcommand{\pderw}[2]{\frac{\partial^2 #1}{\partial #2^2}}$
    $\newcommand{\pderws}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\inner}[2]{\langle #1, #2 \rangle}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\nullspace}[1]{\mathcal{N}\pare{#1}}$
    $\newcommand{\range}[1]{\mathcal{R}\pare{#1}}$
    $\newcommand{\var}[1]{\text{var}\pare{#1}}$
    $\newcommand{\cov}[2]{\text{cov}(#1, #2)}$
    $\newcommand{\tr}[1]{\text{tr}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$
    $\newcommand{\ipto}{\overset{\text{i.p.}}\longrightarrow}$
    $\newcommand{\asto}{\overset{\text{a.s.}}\longrightarrow}$
    $\newcommand{\expdist}[1]{\text{ ~ Exp}(#1)}$
    $\newcommand{\unifdist}[1]{\text{ ~ Unif}(#1)}$
    $\newcommand{\normdist}[2]{\text{ ~ N}(#1,#2)}$
    $\newcommand{\berndist}[2]{\text{ ~ Bernoulli}(#1,#2)}$
    $\newcommand{\geodist}[1]{\text{ ~ Geometric}(#1)}$
    $\newcommand{\poissondist}[1]{\text{ ~ Poisson}(#1)}$
    $\newcommand{\ceil}[1]{\lceil#1\rceil}$
    $\newcommand{\floor}[1]{\lfloor#1\rfloor}$
    $\newcommand{\Re}[1]{\text{Re}(#1)}$
    $\newcommand{\Im}[1]{\text{Im}(#1)}$
    $\newcommand{\dom}[1]{\text{dom}(#1)}$
    $\newcommand{\fnext}[1]{\overset{\sim}{#1}}$
    $\newcommand{\transpose}[1]{{#1}^{\text{T}}}$
    $\newcommand{\b}[1]{\boldsymbol{#1}}$
    $\newcommand{\None}[1]{}$
    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$
    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Stochastic process" status="" %}}

A stochastic process is a mathematical model of a probabilistic experiment that evolves in time and generates a sequence of numerical values. Each numerical value in the sequence is modeled by a random variable, so a stochastic process is simply a (finite or infinite) sequence of random variables.

However, stochastic process involve some change in emphasis over our earlier models. In particular:

1. more focus on the **dependencies** in the sequence of values generated by the process. <br>
e.g. how do future prices of a stock depend on past values?
2. more interest in **long-term averges** involving the entire sequence of generated values.<br>
e.g. what is the fraction of time that a machine is idle?
3. interest in characterizing the likelihood or frequency of certain **boundary events**.<br>
e.g. what is the probability that within a given hour all circuits of some telephone system become simultaneously busy?

{{% /definition %}}

{{% example name="Stochastic processes" %}}

1. the sequence of daily prices of a stock
2. the sequence of scores in a football game
3. the sequence of failure times of a machine
4. the sequence of hourly traffic loads at a node of a communication network

{{% /example %}}

{{% definition name="Arrival-Type Processes" status="" %}}

**Arrival-Type Processes** are processes where occurrences have the character of an "arrival".

We will focus on models where the interarrival times (the times between successive arrivals) are independent random variables.

The case where arrivals occur in discrete time and the interarrival times are <u>geometrically</u> distributed is the **Bernoulli process**;

The case where arrivals occur in continuous time and the interarrival times are <u>exponentially</u> distributed is the **Bernoulli process**;

{{% /definition %}}

{{% definition name="Markov Processes" status="" %}}

In Markov processes, experiments evolve in time; future evolution exhibits a probabilistic dependence on the past.

{{% /definition %}}

{{% example name="Markov Processes" %}}

The future daily prices of a stock are typically dependent on past prices.

In a Markov process, we assume a very special type of dependence: the next value
depends on past values only through the current value.

{{% /example %}}

{{% definition name="Bernoulli Process" status="" %}}

A sequence of $X\_1, X\_2, ..., X\_n$ of **independent** Bernoulli random variables
$X\_i $ with

$$P(X\_i = 1) = P(\text{success at the }i\text{th trial}) = p$$
$$P(X\_i = 0) = P(\text{faiure at the }i\text{th trial}) = 1 - p$$

{{% /definition %}}

{{% recall name="Binomial Distribution" %}}

The distribution of the number $S $ of successes in $n $ independent trials.

$$p\_S(k) = \binom{ n }{ k } p^k (1- pk)^{ n - k }, k = 0, 1, ..., n $$

$$E[S] = np, \var{ S } = np(1 - p)$$

{{% /recall %}}

{{% recall name="Geometric Distribution" %}}

The distribution of the number $T $ of trial up to (and including) the first
success.

$$p\_T(t) = (1 - p)^{t -1} p, t = 1,2, ... $$

$$E[T] = \frac{ 1 }{ p }, \var{ T } = \frac{ 1-p }{ p^2 } $$

{{% /recall %}}

{{% remarks name="Independence and Memorylessness" %}}

The independence assumption underlying the Bernoulli process implies the memorylessness property, which, in a nutshell, means:

Whatever has happened in past trials provide no information on the outcomes of future trials.

{{% /remarks %}}

{{% recall name="" %}}

If two random variables $U $ and $V $ are independent, then any two functions of them $g(U) $ and $h(V) $ are also independent.

{{% /recall %}}

{{% example name="" %}}

Define random variable $Y = (X\_1 + X\_3) X\_6 X\_7, Z = (X\_2 + X\_4) (X\_5 + X\_8) $. Because $X\_i $ are independent for all $i $. Since the two collection $\set{ X\_1, X\_3, X\_6, X\_7 } $ and $\set{ X\_2, X\_4, X\_5, X\_8 } $, $Y $ and $Z $ are also independent.

{{% /example %}}

{{% properties name="Fresh-start property of the Bernoulli process" %}}

Suppose a Bernoulli process has been running for $n $ times steps, and that we have observed the value of $X\_1, X\_2, ..., X\_n $. Notice the fact that:

the sequence of future trials $X\_{n+1}, X\_{n+2}, ... $ are independent Bernoulli trials and therefore form a Bernoulli process. In addition, these future trials are independent from the past ones. 

We conclude that starting from any given point in time, the future is also modeled by a Bernoulli process, which is independent of the past. We refer to this loosely as the **fresh-start** property of the Bernoulli process.

More concisely, <br>
for any given time $n $, the sequence of random variable $X\_{n+1}, X\_{n+2}, ...$(the future of the process) is also a Bernoulli process, and is independent from $X\_1, X\_2, ..., X\_n $(the past of the process).

{{% /properties %}}

{{% properties name="Memorylessness property of Geometric Distributions" %}}

Recall that the time $T $ until the first success is a geometric random variable. Suppose that we have been watching the process for $n $ time steps, and no success has been recorded. What can we say about the number $T-n $ of remaining trials until the first success?

Since the future of the process (after time $n $) is independent of the past and
constitutes a fresh-starting Bernoulli process, the number of future trials until the first success is described by the same geometric PMF.

$$P(T-n = t | T> n) = (1-p)^{t-1} p = P(T = t), t = 1, 2, ... $$

We refer to this as the **memorylessness** property.

Note: this can also be derived algebraically, using the definition of conditional probabilities.

More concisely, <br>
Let $n $ be a given time and let $\overline{ T } $ be the time of the first success after time $n $. Then $\overline{ T } - n $ has a geometric distribution with parameter $p $, and is independent of the random variables $X\_1, X\_2, ..., X\_n $.

{{% /properties %}}

{{% example name="" %}}

A computer executes two types of tasks, priority and non priority, and operates in discrete time units (slots). Each task requires one full slot to complete.<br>
A priority task arrives with probability $p $ at the beginning of each slot, independent of other slots, a nonpriority task is always available and is executed at a given slot if no priority task is available.<br>
We call a slot **busy** if within the slot, the computer executes a priority task, and otherwise we all it **idle**. <br>
We call a string of idle slots, **idle period**; and a string of busy slots, **busy period**.<br>
Want to know the PMF, mean, and variance of the following random variables.<br>
1. $T=$ the time index of the first idle slot<br>
2. $B=$ the length (number of slots) of the first busy period<br>
3. $I=$ the length of the first idle period<br>
4. $Z=$ the number of slots after the first slot of the first busy period up to and including the first subsequent idle slot

$T\geodist{1-p}$

Consider the first busy period. It starts with the first busy slot, call it slot $L$. $Z = B\geodist{1-p}$.

$I \geodist{p} $

{{% /example %}}

{{% remarks name="" %}}

Start watching a Bernoulli process at a random time $N$, what we see is indistinguishable from a Bernoulli process that has just started, as long as $N$ is determined only by the past history of the process and does not convey any information on the future.

{{% /remarks %}}

{{% example name="Fresh-start at a Random Time" %}}

Let $N $ be the first time that we have a success immediately following a previous success. (That is, $N $ is the first $i $ for which $X\_{i - 1} = X\_i = 1 $). What is the probability $P(X\_{N+1} = X\_{N+2} = 0) $ that there are no successes in the two trials that follow?

Intuitively, once the condition $X\_{N-1} = X\_N = 1 $ is satisfied, and from then on, the future of the process consists of independent Bernoulli trials. Therefore, the probability of an event that refers to the future of the process is the same as in a fresh-starting Bernoulli process, so that $P(X\_{N+1} = X\_{N+2} = 0) = (1 -p)^2 $.

{{% /example %}}

{{% remarks name="Interarrival Times" %}}

An important random variable associated with the Beronoulli process is the time of the $k $th success (or arrival), which we denote by $Y\_k $. A related random variable is the $k $th interarrival time, denoted by $T\_k $. It is defined by

$$T\_1 = Y\_1, T\_k = Y\_k - Y\_{k-1}, k = 2, 3, ...$$

and represents the number of trials following the $(k-1) $st success until the next success. Note that

$$Y\_k = T\_1 + T\_2 + ... + T\_k $$

Note that $\forall i, j, T\_i, T\_j $ independent.

{{% /remarks %}}

{{% definition name="" status="" %}}

1. Start with a sequence of independent geometric random variables $T\_1, T\_2, ...$ with common parameter $p $, and let these stand for the interval times.

2. Record a success (or arrival) at times, $T\_1, T\_1 + t\_2, T\_1 + T\_2 + T\_3$, etc.

{{% /definition %}}

{{% example name="" %}}

It is observed that after a rainy day, the number of days until it rains again is geometrically distributed with parameter $p $, independent of the past. Find the probability that it rains on both the $5$th and the $8$th day of the month.

View rainy days as "arrivals", then the description of the weather conforms to the alternative description of the Bernoulli process given above.

Any given day is rainy with probability $p$, independent of other days.

In particular, the probability that day $5 $ and $8 $ are rainy is equal to $p^2 $.

{{% /example %}}

{{% definition name="The kth Arrival Time" status="" %}}

$Y\_k$ of the $k$th success (or arrival) is equal to the sum $Y\_k = T\_1 + T\_2 + ... + T\_k$ of $k$ i.i.d. geometric random variables.

With that observation, we can then derive formulas for the mean, variance, and PMF of $Y\_k $, which are given in the table that follow.

{{% /definition %}}


{{% properties name="Properties of the kth Arrival Time" %}}

$$E[Y\_k] = E[T\_1] + ... + E[ T\_k ] = \frac{ k }{ p }$$

$$\var{ Y\_k } = \var{ T\_1 } + ... + \var{ T\_k } = \frac{ k(1-p)}{ p^2 } $$

$$p\_{Y\_k}(t) = \binom{ t-1 }{ k-1 } p^k(1-p)^{t-k} , t = k, k+1, ...$$

This is known as the **Pascal PMF of order $k$**.

{{% /properties %}}

{{% definition name="Splitting of a Bernoulli Process" status="" %}}

Start with a Bernoulli process in which there is a probability $p$ of an arrival at each time, consider **splitting** it as follows. Whenever there is an arrival. We choose to either keep it (with probability $q$), or to discard it (with probability $1-q$).

Then, the probability of a kept arrival is $pq$, and the probability of a discarded arrival is $p(1-q)$. The sequence of kept arrivals and discarded arrivals are both Bernoulli sequences.

{{% /definition %}}

{{% definition name="Merging of two Bernoulli Process" status="" %}}

Start with two Bernoulli processes with parameter $p $ and $q $ respectively, consider **merging** as follows. An arrival is recorded in the merged process if and only if there is an arrival in at least one of the two original process. The probability $P = 1 - (1-p)(1-q) = p + q - pq$.

Since different time slots in either of the original processes are independent, different slots in the merged process are also **independent**. Thus the merged process is Bernoulli with success probability $p+q-pq $ at each time step.

{{% /definition %}}

{{% definition name="The Poisson Approximation to the Binomial" status="" %}}

The number of successes in $n$ independent Bernoulli trials is a binomial random
variable with parameters $n$ and $p$, and its mean is $np$. 

When $n$ is large but $p$ is small, the mean $np$ has a moderate value. This kind
of situations can be addressed by keeping the product $np$ at a constant value
$\lambda$. To do so we let $n$ grow while simultaneously let $p$ decrease.

In the limit, it turns out that the formula for the binomial PMF simplifies to
Poisson PMF, that is

$\forall k > 0 $
$$\begin{align\*}
\limu{ n }{ \infty } p\_S(k) &= \frac{ n! }{(n-k)!k! } \cdot p^k (1-p)^{ n-k } \br
&= e^{- \lambda} \frac{ \lambda^k }{ k! } = p\_Z(k)
\end{align\*}$$

where $S \binomdist{ n }{ p }, Z \poissondist{ \lambda }, p = \lambda / n$

In general, the Poisson PMF is a good approximation to the binomial as long as
$\lambda=np $, $n $ is very large, $p $ is very small.

{{% proof index="" method="" %}}

Let $S \binomdist{ n }{ p }, p = \lambda / n $, then 

$$\begin{align\*}
p\_S(k) &= \frac{ n! }{(n-k)!k! } \cdot p^k(1-p)^{n-k} \br
&= \frac{n(n-1) ...(n-k+1)}{k!}\cdot\frac{\lambda^k}{n^k}\pare{1-\frac{\lambda}{ n }}^{n-k} \br
&= \frac{ n }{ n } \cdot \frac{ n-1 }{ n } ... \frac{ n-k+1 }{ n } \cdot \frac{ \lambda^k }{ k! } \cdot \pare{ 1- \frac{ \lambda }{ n }}^{n-k}
\end{align\*}$$

$\limu{ n }{ \infty } \frac{ n - c }{ n } = 1$, given $c$ is constant.

Furthermore,

$$\pare{ 1- \frac{ \lambda }{ n }}^{-k} \to 1 $$

$$\pare{ 1 - \frac{ \lambda }{ n }}^n \to e^{- \lambda} $$

Then, fix $k > 0 $

$$\limu{ n }{ \infty } p\_S(k) = e^{- \lambda} \frac{ \lambda^k }{ k! }$$

{{% /proof %}}

{{% /definition %}}

{{% note name="" %}}

As a rule of thumb, the Poisson/binomial approximation is valid to several decimal places if $n \geq 100, p \leq 0.01, \lambda = np $.

{{% /note %}}
