---
title: "Derived Distributions"
date: 2018-07-25T10:34:48+08:00
volumes: ["4"]
layout: "note"
type: "notes"
issue: 1
weight: 41

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$
    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\dist}[1]{\text{dist}(#1)}$
    $\newcommand{\max}[1]{\text{max}(#1)}$
    $\newcommand{\min}[1]{\text{min}(#1)}$
    $\newcommand{\supr}[1]{\text{sup}(#1)}$
    $\newcommand{\infi}[1]{\text{inf}(#1)}$
    $\newcommand{\argmax}[1]{\underset{#1}{\text{argmax }}}$
    $\newcommand{\argmin}[1]{\underset{#1}{\text{argmin }}}$
    $\newcommand{\set}[1]{\left\{#1\right\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\tilde}{\text{~}}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\proj}{\text{proj}}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{\left| #1 \right|}$
    $\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$
    $\newcommand{\pare}[1]{\left(#1\right)}$
    $\newcommand{\brac}[1]{\left[#1\right]}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\limu}[2]{\underset{#1 \to #2}\lim}$
    $\newcommand{\der}[2]{\frac{\d #1}{\d #2}}$
    $\newcommand{\derw}[2]{\frac{\d #1^2}{\d^2 #2}}$
    $\newcommand{\pder}[2]{\frac{\partial #1}{\partial #2}}$
    $\newcommand{\pderw}[2]{\frac{\partial^2 #1}{\partial #2^2}}$
    $\newcommand{\pderws}[3]{\frac{\partial^2 #1}{\partial #2 \partial #3}}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\inner}[2]{\langle #1, #2 \rangle}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\nullspace}[1]{\mathcal{N}\pare{#1}}$
    $\newcommand{\range}[1]{\mathcal{R}\pare{#1}}$
    $\newcommand{\var}[1]{\text{var}(#1)}$
    $\newcommand{\tr}[1]{\text{tr}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$
    $\newcommand{\ceil}[1]{\lceil#1\rceil}$
    $\newcommand{\floor}[1]{\lfloor#1\rfloor}$
    $\newcommand{\Re}[1]{\text{Re}(#1)}$
    $\newcommand{\Im}[1]{\text{Im}(#1)}$
    $\newcommand{\dom}[1]{\text{dom}(#1)}$
    $\newcommand{\fnext}[1]{\overset{\sim}{#1}}$
    $\newcommand{\transpose}[1]{{#1}^{\text{T}}}$
    $\newcommand{\b}[1]{\boldsymbol{#1}}$
    $\newcommand{\None}[1]{}$
    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$
    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Derived distribution" status="" %}}

Let $X$ be a continuous random variable with PDF $f\_X(x) : \R \to [0, +\infty)$, and $g: \R \to \R$ be a function.
Want to determine the **derived distribution** $Y = g(X)$.

We first calculate the CDF $F\_Y(y)$

$$F\_Y(y) = \int\_{\set{ x : g(x) \leq y}} f\_X(x) \d x$$

Assuming $F\_Y(y)$ is differentiable, then 

$$f\_Y(y) = \der{F\_Y}{y}(y)$$

{{% /definition %}}

{{% example name="" %}}

$X \tilde \text{Uniform}(-1, 1]$, $Y= X^3$. Find the distribution of $Y$.

$$f\_X(x) = \begin{cases}
  \frac{ 1 }{ 2 } & x \in (-1, 1] \br
  0 & x \not \in (-1, 1]
\end{cases}$$

$$\begin{align}
  F\_Y(y) =& \int\_\set{ x: x^3 \leq y } f\_X(x) \d x
  = \int\_{ -\infty }^{ \sqrt[ 3 ]{y}} f\_X(x) \d x \br
  =& \begin{cases}
  0& y < -1 \br
  \frac{ 1 }{ 2 } \sqrt[ 3 ]{ y } + \frac{ 1 }{ 2 } & y \in (-1, 1] \br
  1 & y > 1
  \end{cases}
\end{align}$$


$$f\_Y(y) =\der{ }{ y }  F\_Y(y) = \begin{cases}
\frac{ 1 }{ 2 } \abs{ \frac{ 1 }{ 3 } y ^{ - \frac{ 2 }{ 3 }}}=\frac{ 1 }{ 6 } y ^{ - \frac{ 2 }{ 3 }} & y \in (-1, 1] \br
0 & y \not \in (-1, 1]
\end{cases}$$

{{% /example %}}

{{% remarks name="Monotonic" %}}

$f: I \to J. I, J \subseteq \R $.

$f$ is **strictly increasing** if $f(x) < f(y), \forall x < y$.

$f$ is **strictly decreasing** if $f(x) > f(y), \forall x < y$.

$f$ is **strictly monotonic** if it is either strictly increasing or strictly decreasing.

{{% /remarks %}}

{{% remarks name="" %}}

$g: I \to J$ is a strictly monotonic function, $\exists h: J \to I$, such that $g(h(y))  = y, y \in J$.

By chain rule, we get

$$g'(h(y)) h'(y) = 1, y \in J$$

and therefore

$$h'(y) = \frac{ 1 }{ g'(h(y))}$$

{{% /remarks %}}

{{% theorem name="derived distribution of monotonic function" index="" status="" %}}

Let $X$ be a continuous random variable such that $F\_X$ is differentiable.

Let $I, J$ be open intervals in $\R$, $g: I \to J$ be a strictly monotonic differentiable function. That is

$$g'(x) > 0\text{ or } g'(x) < 0, \forall x \in I$$

Let $h: J \to I$ be the inverse of $g$. Notice that $h$ is also differentiable.

Then, the PDF of $Y$ in the region where $f\_Y(y) > 0$ is given by

$$f\_Y(y) = f\_X(h(y)) \abs{h'(y)} = f\_X(h(y)) \frac{ 1 }{ \abs{ g'(h(y))}}, \forall y \in J$$

{{% /theorem %}}

{{% note name="" %}}

If $g$ is increasing,

$$\begin{align\*}
F\_Y(y) &= \b{P}(Y \leq y) = \b{P}(g(X) \leq y) = \b{P}(X \leq h(y)) \br
&= \int\_{-\infty}^{h(y)}f\_X(x)\d x \br
&= F\_X(h(y))
\end{align\*}$$

$f\_Y(y) = \der{}{y}F\_X(h(y)) = f\_X(h(y))h'(y)$.

If $g$ is decreasing,

$$\begin{align\*}
F\_Y(y) &= \b{P}(Y \leq y) = \b{P}(g(X) \leq y) = \b{P}(X \geq h(y)) \br
&= 1 - F\_X(h(y))
\end{align\*}$$

$f\_Y(y) = \der{}{y} (1- F\_X(h(y))) = - f\_X(h(y))h'(y)$.

{{% /note %}}

{{% example name="" %}}

$T\_A \tilde \text{Exp}(\lambda), T\_B \tilde \text{Exp}(\mu). \lambda, \mu > 0.$ Suppose $T\_A, T\_B$ are independent. $X = \max{ T\_A, T\_B }, Y = \min{ T\_A, T\_B }$. Want to know $F\_X, F\_Y $.

$\begin{align\*}
F\_X(y)=& \b{P}(\max{ T\_A, T\_B } \leq y) \br
=& \b{P}(T\_A \leq y, T\_B \leq y) \br
=& \b{P}(T\_A \leq y) \b{P}(T\_B \leq y) \br
=& F\_{T\_A}(y)F\_{T\_B}(y) \br
=& (1 - e^{- \lambda y})(1 - e^{ - \mu y })
\end{align\*}$

$f\_X(y) = \der{  }{ y }F\_X(y) $

$\begin{align\*}
&\b{P}(\min{ T\_A, T\_B } \leq y) \br
=& 1 - \b{P}(\min{ T\_A, T\_B } \geq y) \br
=& 1 - \b{P}(T\_A \geq y, T\_A \geq y) \br
=& 1 - P (T\_A \geq y) P (T\_B \geq y) \br
=& 1 - (1 - F\_{T\_A}(y))(1 - F\_{T\_B}(y)) \br
=& \begin{cases}
  1 - e^{-y \lambda}e^{-y \mu} = 1 - e ^{- (\lambda + \mu)y}, y \geq 0
\end{cases} 
\end{align\*}$

Note, $Min (T\_A, T\_B) ~ Exponential (\lambda + \mu)$

$E(X) = E(T\_A + T\_B - \min{ T\_A, T\_B }) = \frac{ 1 }{ \lambda } + \frac{ 1 }{ \mu } - \frac{ 1 }{ \lambda + \mu }$

$E(Y) = \frac{ 1 }{ \lambda + \mu } $


$X = \max{ T\_A, T\_B }$

$Y = \min{ T\_A, T\_B } $

$T\_A - T\_B - independent $

$F\_Y(y) = \iint\_{ \set{(a, b): \min{(a, b)} \leq y }} f\_{T\_AT\_B}(a, b) \d a \d b  = \iint\_{ \set{(a, b): \min{(a, b)} \leq y }}f\_{T\_A}(a) f\_{T\_B}(b) \d a \d  b$

$\begin{cases}
F\_Y(y) &= \b{P}(\min{ T\_A T\_B } \leq y)  = 1 - \b{P}(\min{  T\_A, T\_B } \geq y) \br
&= 1 - \b{P}(T\_A \geq y, T\_B \geq y) = 1- \b{P}(T\_A \geq y)\b{P}(T\_B \geq y) \br
&= 1 - (1 - F\_{T\_A}(y))(1 - F\_{T\_B}(y))
\end{cases}$ 

{{% /example %}}

{{% example name="" %}}

Suppose $X,Y $ are two independent integratable random variable. Then the probability mass function of $x + y $ is given by 

for $t \in \Z,$

$\begin{align\*}
P\_{X+Y}(t)  \br
=& \b{P}(X+Y = t)  \br
=& (TPF) \sum\_{ x \in \Z } \b{P}(X = x) \b{P}(X + Y = t | X = x)  \br
=& \sum\_{ x \in \Z } \b{P}(X = x) \b{P}(Y = t - x | X = x)  \br
=& \sum\_{ x \in \Z } \b{P}(X = x) \b{P}(Y = t - x)  \br
=& \sum\_{ x \in \Z } P\_X(x)P\_Y(t- x)
\end{align\*}$


{{% /example %}}


{{% theorem name="" index="" status="" %}}

Let $X, Y $ be two independent random variable, such that$F\_{X+Y}(t)$ is differentiable $\forall t \in R$. Then

$$f\_{X+Y}(t) = f\_X *f\_Y(t) \forall t \in \R $$

{{% proof index="" method="" %}}

$F\_{X+Y}(t) = \b{P}(X +Y \leq t) = \iint\_{ \set{(x, y) \in \R, x + y \leq t }} f\_{X, Y}(x, y) \d x \d  y == \iint\_{ \set{(x, y) \in \R, x + y \leq t }} f\_X(x)f\_Y(y) \d x \d y = \int\_{ x \in \R } \int\_{ -\infty }^{ t - x } f\_X(x) f\_Y(y) \d y \d x$

$f\_{X+Y}(t) = \der{  }{ t } F\_{X+Y} (t) = \der{  }{ t } \int\_{ x \in \R } \int\_{ -\infty }^{ t - x } f\_X(x) f\_Y(y) \d y \d x = \int\_{ x \in \R } f\_X(x) \der{  }{ t } \int\_{ -\infty }^{ t - x } f\_Y(y) \d y \d x=  \int\_{ x \in \R } f\_X(x) f\_Y(t - x) \der{  }{ t }(t - x) \d x =  \int\_{ x \in \R } f\_X(x)  f\_Y(t - x) \d x  = f\_X*f\_Y(t)$

{{% /proof %}}

{{% /theorem %}}

{{% definition name="Convolution" status="" %}}

**(discrete case)**

Let $Z = X + Y $, where $X$ and $Y$ are independent integer-valued random variables with PMFs $p\_X $ and $p\_Y $, respectively. Then, for any integer $z $,

$$\begin{align\*}
  p\_Z(z) &= \b{P}(X + Y = z) \br
  &= \sum\_{ \set{(x, y) | x + y = z }} \b{P}(X = x, Y = y) \br
  &= \sum\_{ x } \b{P}(X = x, Y = z - x) \br
  &= \sum\_{ x } p\_X(x)p\_Y(z - x)
\end{align\*}$$

The resulting PMF $p\_z $ is called the **convolution** of the PMFs of $X  $ and $Y $.

**(continuous case)**

Suppose now that $X $ and $Y $ are independent continuous random variables with PDFs $f\_X$ and $f\_Y $, respectively. Then, to find the PDF of $Z $, we first note that

$$\begin{align\*}
  \b{P}(Z \leq z | X = x) =& \b{P}(X + Y \leq z | X = x) \br
  =& \b{P}(x + Y \leq z | X = x) \br
  =& \b{P}(x + Y \leq z) \br
  =& \b{P}(Y \leq z - x)
\end{align\*}$$

By differentiating both sides with respect to $z$, we see that $f\_{Z|X}(z|x) = f\_Y(z - x) $. Using the multiplication rule, we have

$$\begin{align\*}
f\_{X, Z}(x, z)  =& f\_X(x) f\_{Z|X}(z|x) \br
=& f\_X(x)f\_Y(z- x)
\end{align\*}$$

Then,

$$\begin{align\*}
f\_Z(z) =& \int\_{ -\infty }^{ +\infty } f\_{X, Z} (x, z) \d x \br
=& \int\_{ -\infty }^{ +\infty } f\_X(x) f\_Y(z - x) \d x
\end{align\*}$$

{{% /definition %}}

