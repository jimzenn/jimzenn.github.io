---
title: "Gram-Schmidt Orthogonalization process"
date: 2018-07-31T12:00:48+08:00
volumes: ["6"]
layout: "note"
type: "notes"
issue: 2
weight: 602

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{| #1 |}$
    $\newcommand{\proj}{\text{proj}}$
    $\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$
    $\newcommand{\pare}[1]{\left\(#1\right\)}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\limu}[2]{\underset{#1 \to #2}\lim}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\inner}[2]{\langle #1, #2 \rangle}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\tr}[1]{\text{tr}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$
    $\newcommand{\Re}[1]{\text{Re}(#1)}$
    $\newcommand{\Im}[1]{\text{Im}(#1)}$


    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Orthonormal Basis" status="" %}}

Let $V $ be an inner product vector space over a field $F = \R $ or $\C$.

Then a basis $\beta $ of $V $ is called an **orthonormal basis** of $V $ if $\beta $ is an ordered basis and $\beta $ is an **orthonormal set** .

{{% /definition %}}

{{% theorem name="" index="6.3" status="" %}}

Let $V $ be an inner product vector space over a field $F = \R $ or $\C$.

Let $S = \set{   v\_1,  v\_2, ...,  v\_{ k }} $ be a orthogonal set of non-zero vectors. Let $v $ be an vector in $V $ s.t. $v \in \spa{ S } $. Then

$$v = \sum\_{i = 1}^{k} \frac{ \inner{ v }{ v\_i }}{ \norm{ v\_i }^2 } v\_i$$

{{% proof index="" name="" %}}

$\because v \in \spa{ S } \exists $

$a\_1, \_2, ..., \_{ k } \in s.t. $

$\begin{align\*}
v &= a\_1v\_1 + a\_2v\_2 + ... + a\_{ n }v\_{ n } \br
&= \sum\_{ i=1 }^{ n }a\_iv\_i \br
\end{align\*}$

Solve for $a\_1 $ of both sides:

Take inner product $\inner{ \cdot }{ v\_1 } $

$\inner{ v\_1 }{ v\_1 } = \inner{ \sum\_{ i=1 }^{ k }a\_iv\_i }{ v\_1 }$

$\begin{align\*}
\inner{ v\_1 }{ v\_1 } &= \sum\_{ i=1 }^{ k } \inner{ v\_i }{ v\_1 } \br
&= a\_1 \inner{ v\_1 }{ v\_1 } (\because S \text{ is orthogonal}) \br
&= a\_1 \norm{ v\_1 }^2
\end{align\*}$

$\implies a\_1 = \frac{ \inner{ v\_1 }{ v\_1 }}{ \norm{ v\_1 }^2 } (\because v\_1 \neq 0)$

Similarly, to solve for $a\_i$, take inner product $\inner{ \cdot }{ v\_i } $.

$$a\_i = \frac{ v, v\_i }{ \norm{ v\_i }^2 }, \forall i = 1, 2, ..., k $$

Now, $v = \sum\_{ i-1 }^{ k } a\_iv\_i = \sum\_{ i=1 }^{ k } (\frac{ \inner{ v }{ v\_i }}{ \norm{ v\_i }^2 })v\_i $

{{% /proof %}}

{{% /theorem %}}

{{% corollary name="" index="" %}}

If $S = \set{ v\_1, v\_2, ..., v\_{ k }}$ is an orthonormal set, then for any $v \in \spa{ S }$,

$$v = \sum\_{ i = 1 }^{ k } \inner{ v }{ v\_i }v\_i $$

{{% proof index="" name="" %}}

since $S = \set{ v\_1, v\_2, ..., v\_{ k }}$ is an orthonormal set, $\norm{ v\_i } = \norm{ v\_i } = 1, 2, ..., k $

$\therefore$ it follows from $(1) $

{{% /proof %}}

{{% /corollary %}}

{{% corollary name="" index="" %}}

Let $V $ be an inner product vector space over a field $F = \R $ or $\C$, and $S$ is a orthogonal set of non-zero vectors. Then $S $ is linearly independent.

{{% proof index="" name="" %}}

Let $v\_1, \_2, ..., \_{ k } $ be same arbitrary vectors in $S $. Consider the equation $a\_1v\_1 + a\_2v\_2 + ... + a\_{ k }v\_{ k } = 0 $.


$$\begin{align\*}
\inner{ a\_1v\_1 + a\_2v\_2 + ... + a\_{ k }v\_{ k }}{ v\_1 } &= \inner{ 0 }{ v\_1 } \br
a\_1 \inner{ v \_1} {v\_1}+ a\_2 \inner{ v \_2}{v\_1} + ... + a\_{ k } \inner{ v \_{ k }}{v\_1} &= 0 \br
a\_1 \norm{ v\_1 }^2 &= 0
\end{align\*}$$

$\because $ the vectors in $S $ are non-zero vectors by hypotheses $\norm{ v\_1 } \neq 0 $

$\therefore a\_1 = 0 $

Similarly, $a\_2 = 0 , ...,  a\_k = 0$

$\therefore, S $ is linearly independent.


{{% /proof %}}


{{% /corollary %}}

{{% theorem name="Gram-Schmidt Orthogonalization process" status="" %}}

Let $V $ be an inner product vector space over a field $F = \R $ or $\C$, and $S = \set{ w\_1, w\_2, ..., w\_{ n }}$  be a linearly independent set.

Define a set $S' = \set{ v\_1, v\_2, ..., v\_{ n }} $ in the following way:


$$\begin{align\*}
v\_1 &= w\_1 \br
v\_k &= w\_k - \sum\_{ i=1 }^{ k-1 } \frac{ \inner{ w\_k }{ v\_i }}{ \norm{ v\_i }^2 } v\_i, \forall k = 2,3,...n
\end{align\*}$$

Then $S' $ is an <u>orthogonal set of non-zero vectors</u> and $\spa{ S' } = \spa{S} $.

{{% proof index="" name="" %}}

For $n = 1, S =  \set{ w\_1 } ,  S' = \set{ v\_1 } = \set{ w\_1 }$

Then $S'$ is orthogonal and $\spa{ S } = \spa{ S' } $.

Assume that the statement is true for $n = k - 1 $, i.e. $S'\_{k -1} = \set{ v\_1 ... v\_k }$ is constructed using the algorithm satisfies the conclusion.

$S\_k = \set{ w\_1, w\_2, ..., w\_{ k }} = S\_{k-1} \cup \set{ w\_k } $,

$S'\_k  = \set{ v\_1, v\_2, ..., v\_{ k }} = s'\_{k-1} \cup \set{ v\_k } $.

$v\_k = w\_k - \sum\_{ i=1 }^{ k-1 } \frac{ \inner{ w\_k }{ v\_i }}{ \norm{ v\_i }^2 } v\_i $

If $v\_k = 0 $, then $w\_k = \sum\_{ i=1 }^{ k-1 } \frac{ \inner{ w\_k }{ v\_i }}{ \norm{ v\_i }^2 } v\_i $

$\implies w\_k \in \spa{ v\_1, v\_2, ..., v\_{ k-1 }} = \spa{ S'\_{k-1}} = \spa{ S\_{k-1}} $

i.e. $w\_k \in \spa{ S\_{k-1}} $

A contradiction, since $s\_k = s\_{k-1} \cup \set{ w\_k }$ is linearly independent.

$v\_j \in S'\_{k -1} = \set\_{ v\_1, v\_2, ..., v\_{ k-1 }} $

$\inner{ v\_k }{ v\_j } = \inner{ w\_k }{ v\_j } - \sum\_{ i = 1 }^{ k } \frac{ \inner{ w\_k }{  v\_i }}{ \norm{ v\_i }^2 } \inner{ v\_i }{ v\_j } =  \inner{ w\_k }{ v\_j } - \frac{ \inner{ w\_k }{ v\_i }}{ \norm{ v\_i }^2 } \norm{ v\_i }^2 \br= \inner{ w\_k }{ v\_j } - \inner{ w\_k }{ v\_j } = 0$

We know

$v\_1 = w\_1 \in \spa{ S\_k } $

$v\_2 = w\_2 - \frac{ \inner{ w\_2  }{ v\_1 }}{ \norm{ v\_1 }^2 } v\_1 \in \spa{ S\_k } $

Since $S\_k $ is linearly independent, \dim \spa{ S\_k } = k.

Since $S'\_k $ is an orthogonal set of non-zero vectors and $\abs{ S'\_k } = k $

$\therefore \dim \spa{ S'\_k } = k = \dim \spa{ S\_k } $

$\implies \spa{S'\_k} = \spa{ S\_k } $


{{% /proof %}}


{{% /theorem %}}

{{% note name="what happens in an induction if we don't check the base case?" status="extra" %}}

Prove that $n^2 + 5n + 6 $ is an odd integer for all $n \geq 1 $.

Suppose we do not check the base case.

Assume for $n = k, k^2 + 5k + 6 $ is odd. Then


$$\begin{align\*}
(k+1)^2 + 5(k +1) + 6 &= K^2 +2k +1 + 5k + 5 + 6 \br
&= (k^2 +5k + 6) + 2k + 6 \br
&= m + 2(k +3)
\end{align\*}$$

Since $m$ is odd, $2(k +3) $ is even, $(k+1)^2 + 5(k +1) + 6 $ is therefore odd?

NO, because the base case is **FALSE**.

{{% /note %}}

{{% example name="" %}}

$\R^3 $. $S = \set{(1, -1, 0), (1, 0, 1), (0, -1, 1)} $ is a basis of $\R^3 $, Starting from $S $, and apply Gram-Schmidt process to construct an orthonormal basis of $\R^3 $.

$w\_1 = (1, -1, 0), w\_2 = (1, 0, 1), w\_3 = (0, -1, 1) $

$v\_1 = w\_1 = (1, -1, 0) $

$v\_2 = w\_2 - \frac{ \inner{ w\_2 }{ v\_1 }}{ \norm{ v\_1 }^2 } v\_1 = (\frac{ 1 }{ 2 }, \frac{ 1 }{ 2 }, 1)$

$v\_3 = w\_3 - \frac{ \inner{ w\_3 }{ v\_1 }}{ \norm{ v\_1 }^2 } v\_1 - \frac{ \inner{ w\_3 }{ v\_2 }}{ \norm{ v\_2 }^2 } v\_2 = (- \frac{ 2 }{ 3 }, - \frac{ 2 }{ 3 }, \frac{ 2 }{ 3 })$

$\beta = \set{ \frac{ v\_1 }{ \norm{ v\_1 }}, \frac{ v\_2 }{ \norm{ v\_2 }}, \frac{ v\_3 }{ \norm{ v\_3 }}} $

{{% /example %}}
