---
title: "Composition of Linear Transformations and Matrix Multiplication"
date: 2018-07-18T11:03:48+08:00
volumes: ["2"]
layout: "note"
issue: 3
weight: 203

---


<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$ $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$

    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{|#1|}$
    $\newcommand{\inv}[1]{#1^{-1}}$
    $\newcommand{\t#1}{\text}[1]$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$

    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\vcw}[2]{\begin{matrix} #1 \br #2 \end{matrix}}$
    $\newcommand{\vce}[3]{\begin{matrix} #1 \br #2 \br #3 \end{matrix}}$
    $\newcommand{\vcr}[4]{\begin{matrix} #1 \br #2 \br #3 \br #4 \end{matrix}}$
    $\newcommand{\vct}[5]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \end{matrix}}$
    $\newcommand{\vcy}[6]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{matrix}}$
    $\newcommand{\vcu}[7]{\begin{matrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{matrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix}#1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix}#1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix}#1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix}#1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix}#1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix}#1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix}#1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix}#1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>
{{% theorem name="" index="2.9" %}}

$T:V \to W, U: W \to Z$. $V, W, Z$ are vector spaces over $F$, then $U \circ T = UT:V \to Z$ is also a linear transformation.

{{% proof index="" name="" %}}

$x, y \in V, a \in F$.

$\begin{align\*}
UT(ax + y) &= U(T(ax + y)) \br
 &=(aT(x) + T(y)) \br
 &=aU(T(x)) + U(T(y)) \br
 &=a(UT)(x) + (UT)(y)
\end{align*}$

{{% /proof %}}

{{% /theorem %}}

{{% theorem name="" index="2.10" %}}

$V$ is a vector space over a field $F$. Let $T, U\_1, U\_2 \in \L(V)$, then the following holds

1. $T(U\_1 + U\_2) = TU\_1 + TU\_2$
2. $(U\_1 + U\_2)T = U\_1T + U\_2T$
3. $T(U\_1U\_2) = (TU\_1)U\_2$
4. $TI = IT = T$(where $I$ is the identity linear transformation)
5. $a(U\_1U\_2) = (aU\_1)U\_2 = U\_1(aU\_2), \forall a \in F$

{{% proof index="(1)" name="" %}}
$\begin{align\*}
&T(U\_1 + U\_2)(x) \br
&=T((U\_1+U\_2)(x)) \br
&=T(U\_1(x)+U\_2(x)) \br
&=T(U\_1(x))+T(U\_2(x)) \br
&=(TU\_1)(x)+(TU\_2)(x) \forall x \in V
\end{align*}$

$\therefore T \circ (U\_1 + U\_2) = TU\_1 + TU\_2$

{{% note name="" %}}

This is special to linear transformation and does not apply to general functions.

e.g. $f(x) = x^2, g\_1(x) = x^3, g\_2(x)=x^4$ does not follow this theorem.

{{% /note %}}

{{% /proof %}}

{{% /theorem %}}

{{% definition name="Matrix Multiplication" status="star" %}}

Let $A $ be an $m \times n $ matrix and $B $ be an $n \times p $ matrix. We define the product of $A $ and $B $, denoted $AB $, to be the $m \times p $ matrix s.t.

$$(AB)\_{ij} = \sum\_{ k=1 }^{ n } A\_{ik}B\_{kj}, \text{ for } i \leq i \leq m, 1 \leq j \leq p$$

{{% proof name="" %}}

**Analysis**

$T: V \to W, U: W \to Z / F$.

Let
$\alpha = \set{v\_1, v\_2, ..., v\_{n}},
\beta = \set{w\_1, w\_2, ..., w\_{m}},
\gamma = \set{z\_1, z\_2, ..., z\_{p}}$
be the ordered basis of $V, W, Z$ respectively.

Let $A = [U]^\gamma\_\beta$ and $B=[T]^\beta\_\alpha$.

Let $A = (a\_{ij})\_{p \times m}$ and $B = (b\_{ij})\_{m \times n}$.

We define the product of $A$ and $B$ denoted by $AB$ as the new matrix obtained from the matrix representation of $UT$ with respect to  ordered basis $\alpha$ and $\gamma$, i.e. $AB:= [UT]^\gamma\_\alpha$.

We want to find the matrix representation of $UT$.

$UT(v\_j) = \sum\_{i=1}^{v} (\cdot)\_{ij}z\_i$

**Proof**

Let $A=[U]^\gamma\_\beta$ and $B=[T]^\beta\_\alpha$.

$A = (a\_{ij})\_{p\times m},$

$B=(b\_{ij})\_{m \times n}.$

We want to compute the matrix representation of $UT$.


$$\begin{align}
U(w\_j) &= \sum\_{i=1}^{p}a\_{ij}z\_i  & (1)\br
T(v\_j) &= \sum\_{i=1}^{m} b\_{ij} w\_i & (2)
\end{align}$$



$\begin{align\*}
(UT)(v\_j)&=U(T(v\_j)) \br
&= U(\sum\_{i=1}^{m}b\_{ij}w\_i) \br
&= \sum\_{i=1}^{m}b\_{ij}U(w\_i) \br
&= \sum\_{k=1}^{m}b\_{kj}U(w\_k) \br
&= \sum\_{k=1}^{m}b\_{kj}(\sum\_{i=1}^{p} a\_{ik} z\_i)\br
&= \sum\_{i=1}^{p} (\sum\_{k=1}^{m} a\_{ik}b\_{kj})z\_i
\end{align*}$

{{% /proof %}}

{{% /definition %}}


{{% theorem name="" index="2.11" %}}

$T:V \to W, U: W \to Z$,

$\alpha, \beta, \gamma$ are ordered basis of $V, W$ and $Z$ respectively, then

$$[UT]^\gamma\_\alpha = [U]^\gamma\_\beta[T]^\beta\_\alpha$$

{{% proof index="" name="" %}}

Follows from the previous computation

{{% /proof %}}

{{% /theorem %}}

{{% corollary name="" index="" %}}

Let $V $ be a finite-dimensional vector space with an ordered basis $\beta $. Let $T, U \in \L(V)$, then

$$[UT]\_ \beta = [U]\_ \beta [T]\_ \beta $$

{{% /corollary %}}

{{% theorem name="2.14" index="" %}}

Let $V, W $ be two finite-dimensional vector spaces. $T: V \to W$. Let $\beta$ and $\gamma$ be ordered basis $V $ and $W $, respectively, then

$$[ T ]\_{ \beta }^{ \gamma } [ v ]\_{ \beta } = [ T(v) ]\_{ \gamma }, \forall v \in V$$

{{% proof index="" name="" %}}

Fix $v \in V$, let $f: F \to V $ and $g: F \to W $ be two linear transformation.

defined as $f(a) = av, \forall a \in F$ <br>
and $g(a) = aT(v), \forall a \in F$

$g(a) = aT(v) = T(av) = T(f(a)) = (T \circ f)(a), \forall a \in F$

$\therefore, g = T \circ f ... (i) $

Let $\alpha = \set{ 1 }$ be the ordered basis over F.

$[ g ]\_{ \alpha }^{ \gamma } = $

$g(1) = T(v) \implies [ g ]\_{ \alpha }^{ \gamma } = [ T(v) ]\_{ \gamma } ... (iii)$

$f(1) = v \implies [ f ]\_{ \alpha }^{ \beta } = [ v ]\_{ \beta } ... (ii) $

$\underset{ \alpha }{ F } \overset{f}{\to} \underset{ \beta }{ V } \overset{T}{\to} \underset{ \gamma }{ W } $

$g = T \circ f $

$[ g ]\_{ \alpha }^{ \gamma } = [ T \circ f ]\_{ \alpha }^{ \gamma } $

$[ T(v) ]\_{ \gamma }^{ \alpha } = [ T ]\_{ \beta }^{ \gamma } [ f ]\_{ \alpha }^{ \beta } $

$[ T(v) ]\_{ \gamma } =  [ T ]\_{ \beta }^{ \gamma } [ v ]\_{ \beta } $

{{% /proof %}}

{{% /theorem %}}

{{% definition name="left-multiplication transformation" status="extra" %}}

Let $A $ be an $m \times n $ matrix with entries from a field $F $. We denote by $L\_A $ the mapping $L\_A: F^n \to F^m $ defined by $L\_A(x) = Ax $(the matrix product of $A $ and $x $) for each column vector $x \in F^n $. We call $L\_A $ a **left-multiplication transformation**.

{{% /definition %}}

{{% theorem name="" index="2.15" status="extra" %}}

Let $A $ be an $m \times n $ matrix with entries from $F $. Then the left-multiplication transformation $L\_A: F^n \to F^m $ is linear. Furthermore, if $B $ is any other $m \times n $ matrix (with entries from $F $), and $\beta$ and  $\gamma $ are the standard ordered bases for $F^n $ and $F^m $, respectively, then we have the following properties.

1. $[L\_A]^ \gamma \_ \beta = A $
2. $L\_A = L\_B $ if and only if $A = B $
3. $L\_{A+B} = L\_A + L\_B $ and $L\_{aA} = a L\_A $ for all $a \in F$.
4. If $T : F^n  \to F^m $ is linear, then there exists a unique $m \times n $ matrix $C $ s.t. $T = L\_C $. In fact, $C = [ T ]\_{ \beta}^{ \gamma }  $
5. If $E $ is  an $n \times p $ matrix, then $L\_{AE} = L\_A L\_E $
6. If $m = n $, then $L\_{I\_n} = I\_{F^n} $

{{% /theorem %}}

{{% theorem name="associativity of matrix multiplication" index="2.16" status="extra" %}}

Let $A, B $ and $C $ be matrices s.t. $A(BC)$ is defined. Then $(AB)C $ is also defined and $A(BC) = (AB)C $; that is matrix multiplication is **associative**.

{{% /theorem %}}
