---
title: "Diagonalizability"
date: 2018-07-26T12:00:48+08:00
volumes: ["5"]
layout: "note"
type: "notes"
issue: 2
weight: 502

---

<!--more-->

<div class="latex-macros">
  {{< raw >}}
    $\newcommand{\br}{\\}$

    $\newcommand{\R}{\mathbb{R}}$
    $\newcommand{\Q}{\mathbb{Q}}$
    $\newcommand{\Z}{\mathbb{Z}}$
    $\newcommand{\N}{\mathbb{N}}$
    $\newcommand{\C}{\mathbb{C}}$
    $\newcommand{\P}{\mathbb{P}}$
    $\newcommand{\F}{\mathbb{F}}$
    $\newcommand{\L}{\mathcal{L}}$
    $\newcommand{\spa}[1]{\text{span}(#1)}$
    $\newcommand{\set}[1]{\{#1\}}$
    $\newcommand{\emptyset}{\varnothing}$
    $\newcommand{\otherwise}{\text{ otherwise }}$
    $\newcommand{\if}{\text{ if }}$
    $\newcommand{\union}{\cup}$
    $\newcommand{\intercept}{\cap}$
    $\newcommand{\abs}[1]{| #1 |}$
    $\newcommand{\pare}[1]{\left\(#1\right\)}$
    $\newcommand{\t}[1]{\text{ #1 }}$
    $\newcommand{\head}{\text H}$
    $\newcommand{\tail}{\text T}$
    $\newcommand{\d}{\text d}$
    $\newcommand{\inv}[1]{{#1}^{-1}}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\rank}[1]{\text{rank }#1}$
    $\newcommand{\nullity}[1]{\text{nullity}(#1)}$
    $\newcommand{\oto}{\text{ one-to-one }}$
    $\newcommand{\ot}{\text{ onto }}$


    $\newcommand{\Vcw}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Vce}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Vcr}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Vct}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$
    $\newcommand{\Vcy}[6]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \end{bmatrix}}$
    $\newcommand{\Vcu}[7]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \br #6 \br #7 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Mqr}[4]{\begin{bmatrix} #1 & #2 & #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqt}[5]{\begin{bmatrix} #1 & #2 & #3 & #4 & #5 \end{bmatrix}}$

    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mrq}[4]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \end{bmatrix}}$
    $\newcommand{\Mtq}[5]{\begin{bmatrix} #1 \br #2 \br #3 \br #4 \br #5 \end{bmatrix}}$

    $\newcommand{\Mqw}[2]{\begin{bmatrix} #1 & #2 \end{bmatrix}}$
    $\newcommand{\Mwq}[2]{\begin{bmatrix} #1 \br #2 \end{bmatrix}}$
    $\newcommand{\Mww}[4]{\begin{bmatrix} #1 & #2 \br #3 & #4 \end{bmatrix}}$
    $\newcommand{\Mqe}[3]{\begin{bmatrix} #1 & #2 & #3 \end{bmatrix}}$
    $\newcommand{\Meq}[3]{\begin{bmatrix} #1 \br #2 \br #3 \end{bmatrix}}$
    $\newcommand{\Mwe}[6]{\begin{bmatrix} #1 & #2 & #3\br #4 & #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mew}[6]{\begin{bmatrix} #1 & #2 \br #3 & #4 \br #5 & #6 \end{bmatrix}}$
    $\newcommand{\Mee}[9]{\begin{bmatrix} #1 & #2 & #3 \br #4 & #5 & #6 \br #7 & #8 & #9 \end{bmatrix}}$
  {{< /raw >}}
</div>

{{% definition name="Diagonalizability of Linear operators" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

$T$ is **diagonalizable** if there exists an ordered basis $\beta $ of $V $ s.t. $[ T ]\_{ \beta } $ is a diagonal matrix.

{{% /definition %}}

{{% definition name="diagonalizability of matrices" status="" %}}

Let $A\_{n \times n} \in M\_{n \times n}(F)$.

$L\_A : F^n \to F^n $ of $F^n /F$ s.t.

$[ L\_A ]\_{ \gamma }$ is a diagonal matrix.

A square matrix $A $ is called **diagonalizable** if $L\_A $ is diagonalizable.

Equivalently,

$A$ is diagonalizable if exists an invertible matrix  $Q$ s.t. $\inv{ Q }AQ$ is a diagonal matrix.

{{% /definition %}}

{{% remarks name="" %}}

To *diagonalize* a matrix or a linear operator is to find a basis of eigenvectors and the correspoding eigenvalues.

{{% /remarks %}}

{{% example name="" %}}

$L\_A : F^jkn\_{\beta \neq \gamma} \to F^n\_{\beta \neq \gamma} $

$L\_A(v) = Av, \forall v \in F^n $

$v = {\Vcr{ a\_1 }{ a\_2 }{ \vdots }{ a\_n }} \in F^n$

$[ L\_A ]\_{ \beta } = A \if \beta = { e\_1, e\_2, ..., e\_{ n }}$

$[ L\_A ]\_{ \gamma } = B $

$B = \inv{ Q }A Q$

{{% /example %}}

{{% remarks name="" %}}

$T: V \to V $, assume that $T $ is diagonalizable.

$\implies $ ordered basis $\beta = \set{ x\_1, x\_2, ..., x\_{ n }} $ s.t.

$[ T ]\_{ \beta } = {\Mee{ \lambda\_1 }{ 0 }{ 0 }{ 0 }{ \ddots }{ 0 }{ 0 }{ 0 }{ \lambda\_n }} $

$\lambda\_i$ are not necessarily distinct.

$T(x\_1) = \lambda x\_1 + 0 x\_2 + ... + 0 x\_n$

$\implies T(x\_1)  = \lambda \_ 1 x\_1$ and $x\_1 \neq 0$

$\implies \lambda\_1 $is an eigenvalue of $T$ and $x\_1 $ is a corresponding eigenvector.

**Conclusion**: That basis $\beta $ is a basis where every vector in $\beta $ is a eigenvector, and the diagonal entries of $[ T ]\_{ \beta } $ are the corresponding eigenvalues.

{{% /remarks %}}

{{% remarks name="" %}}

$T: V \to V $, assume that $\set{ v\_1, v\_2, ..., v\_{ n }} $ be a basis of $V $ s.t. $v\_i $ is an eigenvector corresponding to eigenvalue of $t\_i $ of $T $.

$$T(v\_1) = t\_1v\_1 = t\_1 v\_1 + 0v\_2 + ... + 0v\_{ n } \br
T(v\_2) = t\_2v\_2 = 0 v\_1 + t\_2v\_2 + ... + 0v\_{ n } \br
\vdots \br
T(v\_n) = t\_nv\_n = 0 v\_1 + 0v\_2 + ... + t\_nv\_{ n }$$

{{% /remarks %}}

{{% theorem name="" index="5.1" %}}
Let $T$ be an linear operator on a finite-dimensional vector space $V$,

Then $T$ is diagonalizable $\iff $

There exists an ordered basis  $\beta = \set{ x\_1, x\_2, ..., x\_{ n }}$.

Consisting of eigenvector of $T $ each $x\_i $ is an eigenvector of $T$. In this case, $[ T ]\_{ \beta }$ is a diagonal matrix with the diagonal entries as corresponding eigenvalue.

{{% /theorem %}}

{{% theorem name="" index="5.5" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

Let $\lambda\_1, \lambda\_2, ..., \lambda\_{ k }$ be distinct eigenvalues of $T , k \leq n$.

If $x\_1, x\_2, ..., x\_{ k } $ are corresponding eigenvector, then $\set{ x\_1, x\_2, ..., x\_{ k }}$ is linearly independent.

{{% proof index="" name="by induction on $k $" %}}

$k = 1, \lambda\_1, x\_1, x\_1 \neq 0 , x\_1$ is linearly independent.

Assume that the statement is true for any $(k-1) $ number of distinct eigenvalue.

Consider:

$a\_1x\_1 + a\_2x\_2 + ... + a\_{ k }x\_{ k } = 0, a\_i \in F$

Apply $(T- \lambda\_a I\_v)$ on both side.


$$\begin{align\*}
(T- \lambda\_k I\_v)(a\_ix\_i) i \neq k \br
&= a\_i (T x\_i - \lambda\_k x\_i) \br
&= a\_i(\lambda\_i x\_i - \lambda\_k x\_i) \br
&= a\_i(\lambda\_i - \lambda\_k)x\_i
\end{align\*}$$

since, $\lambda\_i - \lambda\_k \neq 0$


$a\_1(\lambda\_1 - \lambda\_k)x\_1 + a\_2(\lambda\_2 - \lambda\_k)x\_2 + ... + a\_{ k }(\lambda\_k - \lambda\_k)x\_{ k } = 0, a\_i \in F$

$\therefore a\_i (\lambda\_i - \lambda\_k) = 0 (i = 1, 2, ..., k - 1) \implies a\_i = 0, \lambda\_i \neq \lambda\_k$

using (i) and (ii) we get $a\_kx\_k = 0 $

$\because x\_k $ is an eigenvector. $x\_k \neq 0$ $\therefore \set{ x\_k }$ is linearly independent $\implies a\_k = 0 $


{{% /proof %}}

{{% /theorem %}}

{{% corollary name="" index="" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

$T$ has $n$-distinct eigenvalues $\implies T$ is diagonalizable.

{{% proof index="" name="" %}}

$\lambda\_1 \neq \lambda\_2 \neq ... \neq \lambda\_k$ eigenvalues

$x\_1, $x\_2, ..., $x\_{ n }$ eigenvectors

$\beta = \set{ x\_1, x\_2, ..., x\_{ n }}$ is linearly independent.


Notice $\abs{ \beta } = n = \dim V $

$\therefore \beta$ is a basis of $V $

$\beta $ is a basis of $V$ consisting only eigenvectors of $T $, hence $T $ is diagonalizable.

{{% /proof %}}

{{% /corollary %}}

{{% definition name="Split" %}}

A polynomial $f(x)$ with coefficients in a field F is called **split over** $F$, if $f(x)$ can be written as a product of $n $ linear polynomial with coefficient in $F$.

i.e. $f(x) = a(x- a\_1)(x - a\_2)(x- a\_3) ... (x - a\_n)$

note: $f$ is called a split of over if it can be completely factored into linear factors over $F$.

{{% /definition %}}

{{% example name="" %}}

$x^2 + 2x -3 = (x+3)(x -1) = 5(x-2)^2(x-3)x^3$

{{% /example %}}

{{% example name="" %}}

$5(x-2)(x-3)x^3 = 5(x-2)(x-2)(x-3)xxx$

{{% /example %}}




{{% definition name="algebraic multiplicity" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

Let $\lambda $ be an eigenvalue of $T $, then a positive integer $k \geq 1 $ is called the **algebraic multiplicity** of $\lambda $, if $(\lambda - t)^k $ is a factor of the characteristic polynomial.

{{% /definition %}}

{{% definition name="Eigenspaces" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

$\lambda \in F $ is an eigenvalue of $T $, we define a subspace $E\_ \lambda \subseteq V $ as $E\_ \lambda = \set{ x \in V | T(x) = \lambda x } =N(T - \lambda I\_v)$.

$E\_ \lambda$ is called the **eigenspace** of $\lambda. $

{{% /definition %}}

{{% definition name="geometric multiplicity" %}}

The **geometric multiplicity** of $\lambda$ is defined as the $\dim E \_ \lambda$.

{{% /definition %}}

{{% theorem name="" index="5.7" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$,

$\lambda $ is an eigenvalue of $T $, the algebraic multiplicity of $\lambda $ is $m $, then $1 \leq \dim E\_{ \lambda } \leq m$.

{{% proof index="" name="" %}}

$0 \neq v \in E\_ \lambda \implies \dim E\_ \lambda \geq 1 $.

$\dim E\_\lambda = p$.

Let $\set{ v\_1, v\_2, ..., v\_{ p }}$ be a basis of $E\_\lambda \subseteq V $

Extend this basis to a basis $\set{ v\_1, v\_2, ..., v\_{ p+1 }, v\_{p +1}, v\_{p+2}, ..., v\_{ n }} = \beta$ of $V $.

$[ T ]\_{ beta }$ observe that $v\_1, \_2, ..., \_{ p } $ are all eigenvectors of with respect to $\lambda$.

$\therefore T(v\_1) = \lambda v\_1 = \lambda \cdot v\_1 + 0 \cdot v\_2  + ... + 0 \cdot v\_n$
$T(v\_2) = \lambda v\_2 =0 \cdot v\_1  + \lambda \cdot v\_2 +  ... + 0 \cdot v\_n$
$T(v\_p) = \lambda v\_p =0 \cdot v\_1  + 0 \cdot v\_2 + ... + \lambda \cdot v\_p +   ... + 0 \cdot v\_n$

$T(v\_{p + 1}) = \sum\_{ i=1 }^{ n } a\_{i\_{p+1}}v\_i$

$T(v\_n) = \sum\_{ i=1 }^{ n } a\_{in} v\_i $

$A = [ T ]\_{ \beta } = {\Mww{ \lambda I\_p }{ B }{ 0 }{ C }}$

$(A - tI\_n) = {\Mww{ \lambda I\_p  - t I\_p }{ B }{ 0 }{ C - t I\_{n-p}}}$

$\begin{align\*}
\det(A - tI\_n) &= \det (\lambda I\_p - t I\_p) \det (c - t I\_{n - p}) \br
&= \det ((\lambda - t) I\_p) \det(c- t I\_{n-p}) \br
&= (\lambda -t)^p \det I\_p \det(c - t I\_ {n - p}) \br
&= (\lambda - t)^p g(t), \text{ where } g(t) = \det(c- I\_{n-p})
\end{align\*}$

{{% /proof %}}

{{% /theorem %}}

{{% lemma name="" index="" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$, let $\lambda\_1, \lambda\_2, ..., \lambda\_{ k } $ be distinct eigenvalues of $T $. Let $v\_i \in E\_ { \lambda\_i }$ for $i = 1,2, ... ,k .$

If
$$v\_1 + v\_2 + ... + v\_{ k } = 0 $$
then
$$v\_1 = v\_2 =  ... = v\_k = 0 $$

{{% proof index="" name="" %}}

By contradiction, assume that there is at least one $v\_i$ say $v\_{i\_1} \neq 0 , i \in \set{ 1, 2, ..., k }$,

i.e. $v\_{i\_1} $ is an eigenvector contradiction to $\lambda\_{i\_ 1} $

Let $v\_{i\_1}, v\\_{ i\_2 }, ..., v\\_{ i\_{m}} $ are all non-zero vectors, from $E\_{ \lambda\_{i \_ 1}}, E\_ { \lambda { i\_ 2  }}, ... , E\_ { \lambda { i\_ m  }} $, respectively, and the remaining ones are all zero vectors.

$i\_1, i\_2, ..., i\_{ m } \in \set{ 1, 2, ..., k } $

$\therefore $ the equation $(i) $ looks like the following,

$v\_{i\_1} + v\_{i\_2} + ... + v\_{i\_ m } = 0 $(ii)

Since $v\_{i\_1} \neq 0, v\_{i\_2} \neq 0, ..., v\_{i\_n} \neq 0$.


Then all  eigenvector for  corresponding to distinct eigenvalues, and that is contradiction to (ii) since eigenvectors from different eigenvalues are  linearly independent.

{{% /proof %}}

{{% /lemma %}}

{{% theorem name="" index="5.8" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V, \lambda\_1, \lambda\_2, ..., \lambda\_{ k } $ are distinct eigenvalues of $T $.

Let $S\_i $ be a linearly independent subset of $E\_{ \lambda\_i } $ for $i = 1, 2, ..., k $.

Then $\underset{i=1}{\overset{k} \cup} S\_i$ is also linearly independent.

{{% proof index="" name="" %}}

Let $\dim E\_{ \lambda \_ i  } = n\_i , i - 1, 2, ..., k$

Let $S\_i \subseteq E\_ { \lambda\_ i } $

$S\_i = \set{  v\_{i1}, v \_{i2}, ..., v\_{in\_i}} , i = 1, 2, ..., k$

want to prove $S\_i $ linearly independent.

To that end consider the following equation:

$\sum\_{ i=1 }^{ k } \sum\_{ j = 1 }^{ n\_i } a\_{ij}v\_{ij} = 0$

Let $w\_i  = \sum\_{ j=1 }^{ n\_i } a\_{ij}v\_{ij}, i = 1, 2 , ..., k$

Then $w\_i \in E\_ { \lambda \_ i }$, since $v\_{i1}, v\_{ i2 }, ..., v\_{ in\_i } \in S\_i \subseteq E\_{ \lambda\_ i }$ and $E\_ { \lambda\_i }  is a subspace$

$\therefore $ we get from (i) $\sum\_{ i= 1 }^{ k } w\_i = 0, w\_i \in E\_ {\lambda\_i}$.

By previous theorem we then get

$w\_i = 0, \forall i = 1, 2, ..., k $

i.e. $\sum\_{ j=1 }^{ n\_i } a\_{ij} v\_{ij} = 0 \forall i = 1, 2, ..., k$
since $S\_i = \set{ v\_{i\_1}, v\_{i\_2}, ..., v\_{in\_i}} $ is linearly independent from (iii) we get

$a\_{ij} = 0 \forall i = 1, 2, ..., n and j = 1, 2, ..., n\_i $

$\therefore \underset{i = 1}{\overset{k} \cup} S\_i $ is linearly independent.

{{% /proof %}}

{{% /theorem %}}

{{% theorem name="" index="5.9" %}}

Let $T$ be an linear operator on a finite-dimensional vector space $V$, let $\lambda\_1, \lambda\_2, ..., \lambda\_{ k }$ are distinct eigenvalues of $T$, then

1. $T $ is diagonalizable $\iff \forall \lambda\_i,  $ algebraic multiplicity$(\lambda\_i) =$ geometric multiplicity$(\lambda\_i), i = 1, 2, ..., k $.
2. If $T $ is diagonalizable and if $\beta\_1, \beta\_2, ..., \beta\_{ k } $ are basis of $E\_{ \lambda\_1}, E\_{ \lambda\_2}, ..., E\_{ \lambda\_{ k }} $ respectively, $\beta = \beta\_1 \cup \beta\_2 \cup ... \cup \beta\_k  $  is a basis of $V $.

{{% proof index="" name="" %}}

$T $ is diagonalizable $\iff \exists$  a basis $\beta $ consisting of eigenvectors of $T $.

$\dim V = n $

let $f(t)$  be the characteristic polynomial of $T $.

$deg(f(t))  = n$o
$\lambda\_1, \lambda\_2, ..., \lambda\_{ k }$ are the distinct roots of $f(t)=(\lambda\_1)^{a\_1} + (\lambda\_2)^{a\_2} +  ... +  (\lambda\_k)^{a\_k}$

where $a\_i = $ algebraic multiplicity of $\lambda\_i $

$\therefore deg(f(t)) = n $

$\because a\_1, a\_2, ..., a\_{ k } = n (\*)$

**Part 1** that algebraic multiplicity$(\lambda\_i)$ = geometric multiplicity $(\lambda\_i) , \forall i = 1, 2, ..., k$

$\therefore$ geometric multiplicity $(\lambda\_i) = a\_i, \forall i = 1, 2, ..., k$

Recall that, geometric multiplicity$(\lambda\_i) = \dim E\_ { \lambda\_i }  $

$\therefore$ let $\beta\_i $ be basis of $E\_ { \lambda\_i } $ Then $\abs{ \beta\_i } = a\_i, i = 1, 2, ..., k $

Then $\beta\_1 \cup \beta\_2 \cup ... \cup \beta\_k $ is a linearly independent set by the previous theorem.

Now, observe that $\beta\_i \cap \beta\_j  = \emptyset  $ since $i \neq j $.

$\therefore \abs{ \beta\_1 \cup \beta\_2 \cup ... \cup \beta\_k}  = \abs{ \beta\_1 } + \abs{ \beta\_2 } + ... + \abs{ \beta\_K } = n = \dim V by (\*)$

$\therefore \underset{i = 1}{\overset{k} \cup} \beta\_i $ is a basis of $V $ consisting of eigenvector of $T $. Hence T is diagonalizable.

**Part 2**

Assume that $T $ is diagonalizable. Then exists an ordered basis $\beta $ of $V $ s.t. [ T ]\_{ \beta } is a diagonalizable matrix.

Let $\beta $ = \set{ v\_{11}, v\_{12}, ..., v\_{1n\_1}, v\_{21}, v\_{22}, ..., v\_{2n\_2}, ..., v\_{k1}, v\_{k2}, ..., v\_{kn\_k}}

where , $v\_{i1}, v\_{i2}, ..., v\_{in\_i}$ are eigenvector corresponding to the eigenvalues $\lambda\_i \forall i = 1, 2, ..., k$.


$\therefore $ geo.mult.$(\lambda\_i)  = n\_i$  ... (\*)

$T(v\_{11}) = \lambda\_1v\_{11} $

$\vdots $

$T(v\_{1n\_1}) = \lambda\_1v\_{1n\_i} $

$\vdots $

(you get the idea...)

$\det([ T ]\_{ \beta } - tI\_n) = (\lambda\_1 -t)^{n\_1}(\lambda\_2-t)^{n\_2}...(\lambda\_k -t)^{n\_k}$

$\therefore$ alg.mult.$(\lambda\_i) = n\_i =  $ geo.mult.$(\lambda\_i)$

{{% /proof %}}

{{% /theorem %}}

{{% note name="Test for diagonalization" %}}

Let $T $ be a linear operator on an $n $-dimensional vector space $V $. Then $T $ is diagonalizable if and only if both of the following conditions hold.

For each eigenvalue $\lambda$ of $T $, $\text{alg.mult.}(T) = \text{geo.mult.}(T)$.

These same conditions can be used to test if a square matrix $A $ is diagonalizable because diagonalizability of $A $ is equivalent to diagonalizability of the operator $L\_A $.

If $T $ is diagonalizable, and $\beta\_1, \beta\_2, ..., \beta\_{ k } $ are ordered bases for the eigenspaces of $T $.

Then the union $\beta = \beta\_1 \cup \beta\_2 \cup ... \cup \beta\_k $ is an ordered bases for $V $ consisting of eigenvectors of $T $, and hence $[T]\_ \beta $ is a diagonal matrix.

{{% example name="" %}}

We test the matrix
$$A = {\Mee{ 3 }{ 1 }{ 0 }{ 0 }{ 3 }{ 0 }{ 0 }{ 0 }{ 4 }} \in M\_{3 \times 3}(R\) $$
for diagonalizability.

The characteristic polynomial of $A $ is $\det(A - t I) = 0(t - 4)(t- 3)^2$, which splits.

A has eigenvalues $\lambda\_1 = 4, \lambda\_2 = 3$ with algebraic multiplicities $1$ and $2$, respectively.

Since $\lambda\_1 $ has alg.mult. $1 $, by theorem 5.7, geo.mult. = 1 = alg.mult.

Then we check $\lambda\_2. Since

$$A - \lambda\_2I = {\Mee{ 0 }{ 1 }{ 0 }{ 0 }{ 0 }{ 0 }{ 0 }{ 0 }{ 1 }}$$

has $\rank{ 2 }$, we see that $\nullity{ A - \lambda\_2 I } = 3 - \rank{ A - \lambda\_2 I } = 1 $, which is not the alg.mult. of $\lambda\_2 $.

$\therefore A $ is not diagonalizable.

{{% /example %}}
{{% /note %}}

{{% example name="Finding the $Q$" %}}

Let
$$A = {\Mww{ 0 }{ -2 }{ 1 }{ 3 }} $$
We show that $A $ is diagonalizable and find a $2 \times 2 $ matrix $Q $ s.t. $\inv{ Q }A Q $ is a diagonal matrix.

$A - \lambda I = (t -1)(t- 2) $, and hence $A $ has two distinct eigenvalues, $\lambda\_1 = 1 $ and $\lambda\_2 = 2 $. By applying the corollary to theorem 5.5 to the operator $L\_A $, we see that $A $ is diagonalizable.

Moreover,

$$\gamma\_1 = \set{{\Vcw{ -2 }{ 1 }}}, \gamma\_2 = \set{{\Vcw{ -1 }{ 1 }}} $$

are bases for the eigenspaces $E\_{\lambda\_1} $ and $E\_{ \lambda\_2 } $, respectively. Therefore

$\gamma = \gamma\_1 \cup \gamma\_2 = \set{{\Vcw{ -2 }{ 1 }}, {\Vcw{ -1 }{ 1 }}} $

is an ordered basis for $\R^2 $ consisting of eigenvector of $A $. Let

$$ Q = {\Mww{ -2 }{ -1 }{ 1 }{ 1 }} $$

the matrix whose columns are the vectors in $\gamma$. Then, by the corollary to theorem 2.23,

$$ D = \inv{ Q }AQ = [ L\_A ]\_{ \beta  } = {\Mww{ 1 }{ 0 }{ 0 }{ 2 }} $$


{{% /example %}}
